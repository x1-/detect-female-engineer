#%% Change working directory from the workspace root to the ipynb file location. Turn this addition off with the DataScience.changeDirOnImportExport setting
# ms-python.python added
import os
try:
	os.chdir(os.path.join(os.getcwd(), 'notebooks'))
	print(os.getcwd())
except:
	pass
#%% [markdown]
# # Building, training, and deploying Engineer LSTM Model
#%% [markdown]
# ## Data preparation
# 
# You can download on GCS.
# 
# ```
# gs://your-own-name/model/
# ```

#%%
get_ipython().system('pip freeze')


#%%
deps = """
torch==1.1.0
torchvision
distro
pandas
joblib
numpy
seldon-core
google-cloud-storage
google-compute-engine
google-resumable-media[requests]
fairing==0.5.3
"""
with open("requirements.txt", 'w') as f:
    f.write(deps)
get_ipython().system('pip install -r requirements.txt')

#%% [markdown]
# ## Building a model and training it locally

#%%
import logging
import joblib
import sys
import os
import re
import math
import pandas as pd
import itertools

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.utils.data
from torch.autograd import Variable
from torch.utils.data import TensorDataset, DataLoader


#%%
from gensim import models
from gensim.models.doc2vec import Doc2Vec
from gensim.models.doc2vec import TaggedDocument
import MeCab
import neologdn


#%%
logging.basicConfig(format='%(message)s')
logging.getLogger().setLevel(logging.INFO)

#%% [markdown]
# ## Creating a model class with train and predict methods

#%%
class Predictor(nn.Module):
    def __init__(self, embedding_dim, hidden_dim, tagset_size):
        super(Predictor, self).__init__()

        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim

        # The LSTM takes word embeddings as inputs, and outputs hidden states
        # with dimensionality hidden_dim.
        self.lstm = nn.LSTM(
            input_size = embedding_dim,
            hidden_size = hidden_dim,
            batch_first = True
        )

        # The linear layer that maps from hidden state space to tag space
        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)

    def forward(self, x, h=None):
        lstm_out, _ = self.lstm(x.unsqueeze(1), h)
        tag_space = self.hidden2tag(lstm_out.squeeze(1))
        tag_scores = F.log_softmax(tag_space, dim=1)
        return tag_scores


#%%
class EngineerServe(object):

    def __init__(self):
        self.embedding_dim = 600
        self.hidden_dim = 4
        self.output_dim = 1
        self.learning_rate = 0.001
        self.batch_size = 100
        self.num_epochs = 300

        self.train_input = "train.csv"
        self.gmodel_file = "doc2vec.model"
        self.model_file = "engineer.pt"
        self.trained_model = None


    def tag2(self, df, key):
        d = {}
        for i in range(len(df)):
            name = df.at[i, 'index']
            d[name] = df.at[i, key]
        return d

    def train(self):
        gmodel = Doc2Vec.load(self.gmodel_file)


        df = pd.read_csv(self.train_input)
        df = df[df['sex'].isin([0, 1])]
        ndf = df.reset_index()

        tags = list(ndf['index'])
        tag_to_ix = {"other": 0, "engineer": 1}
        tag2eng = self.tag2(ndf, 'is_engineer')


        train_x = []
        train_y = []
        for tag in tags:
            train_x.append(gmodel.docvecs[tag])
            train_y.append(tag2eng[tag])

        m = len(train_y) % self.batch_size
        train_x_f = torch.FloatTensor(train_x[m:])
        train_y_f = torch.LongTensor(train_y[m:])

        train_dataset = TensorDataset(train_x_f, train_y_f)
        train_loader_total = DataLoader(dataset=train_dataset,
                                        batch_size=self.batch_size,
                                        shuffle=True)

        predictor = Predictor(self.embedding_dim, self.hidden_dim, len(set(tag2eng.values())))
        criterion = nn.NLLLoss()
        optimizer = torch.optim.Adam(predictor.parameters(), lr = self.learning_rate)

        lloss = []
        for epoch in range(self.num_epochs):
            running_loss = 0.0
            for i, (x, y) in enumerate(train_loader_total):
                x = Variable(x, requires_grad=True)
                y = Variable(y)

                optimizer.zero_grad()
                outputs = predictor(x)

                loss = criterion(outputs, y)
                loss.backward()
                optimizer.step()

                running_loss += loss.item()

            if epoch % 10 == 0:
                print ("epoch:{0}, Loss:{1}".format(epoch, loss.item()))

            lloss.append(running_loss)

        torch.save(predictor.state_dict(), self.model_file)
        return lloss


    def predict(self, X, feature_names=None):
        """Predict using the model for given tensor."""
        if not self.trained_model:
            device = torch.device("cpu")
            predictor = Predictor(self.embedding_dim, self.hidden_dim, 2).to(device)
            predictor.eval()
            predictor.load_state_dict(torch.load(self.model_file))
            self.trained_model = predictor

        x = Variable(torch.from_numpy(X)).float()
        outputs = self.trained_model(x)
        prob, predicted = torch.max(outputs.data, 1)

        """Because using LogSoftMax, transform probability"""
        n_prob = math.exp(prob)

        return [n_prob, predicted]

#%% [markdown]
# ## Training Locally

#%%
get_ipython().run_cell_magic('time', '', 'loss = EngineerServe().train()')


#%%
import matplotlib.pyplot as plt
plt.plot(loss, 'ro--')


#%%
import fairing
GCP_PROJECT = fairing.cloud.gcp.guess_project_name()
GCS_BUCKET_ID = "your-own-name"
GCS_BUCKET = "gs://{}/model".format(GCS_BUCKET_ID)
get_ipython().system('gsutil ls {GCS_BUCKET}')


#%%
py_version = ".".join([str(x) for x in sys.version_info[0:3]])
print(py_version)

#%% [markdown]
# # Training and Deploying in Fairing
# 
# ### Setting up base container and builder for fairing
# 
# Setting up google container repositories (GCR) for storing output containers. You can use any docker container registry istead of GCR.

#%%
DOCKER_REGISTRY = 'gcr.io/{}/fairing/engineer'.format(GCP_PROJECT)

base_image = "gcr.io/{}/python3-mecab-pytorch:0.0.3".format(GCP_PROJECT)  # created in ahead.
fairing.config.set_builder('docker', registry=DOCKER_REGISTRY, base_image=base_image)


#%%
DEPLOYMENT_NAME='Your own name'
KUBEFLOW_ZONE='Your own zone'


#%%
get_ipython().system('gcloud config set project $GCP_PROJECT')


#%%
get_ipython().system("gcloud container clusters get-credentials $DEPLOYMENT_NAME --zone $KUBEFLOW_ZONE --project $GCP_PROJECT")


#%%
get_ipython().system('kubectl config set-context $(kubectl config current-context) --namespace=kubeflow')


#%%
get_ipython().system('gcloud auth configure-docker --quiet')

#%% [markdown]
# ## Training in KF

#%%
fairing.config.set_deployer('job')
fairing.config.set_preprocessor("function", function_obj=EngineerServe,
                                input_files=["requirements.txt", "tweets/tweets.csv", "doc2vec.model", "doc2vec.model.trainables.syn1neg.npy", "doc2vec.model.wv.vectors.npy"])
fairing.config.run()

#%% [markdown]
# ## Deploying model and creating an endpoint in KF

#%%
fairing.config.set_preprocessor("function", function_obj=EngineerServe,
                                input_files=["requirements.txt", "engineer.pt"])
fairing.config.set_deployer('serving', serving_class="EngineerServe")
fairing.config.run()


#%%
# Copy the prediction endpoint from prev step
get_ipython().system('curl -g http://xx.xx.xx.xx:5000/predict -H "Content-Type: application/x-www-form-urlencoded" --data-urlencode \'json={"data":{"tensor":{"shape":[1,600],"values":[0.0862196832895279,0.13640227913856506,-0.056235652416944504,0.08986520767211914,0.176371768116951,-0.23108884692192078,0.3020065724849701,-0.08123914897441864,0.0793980062007904,-0.16444535553455353,-0.03293262794613838,0.21857912838459015,0.04735378548502922,-0.19331295788288116,-0.08112654089927673,-0.042077068239450455,0.081704281270504,-0.06857037544250488,0.062000956386327744,0.07896658778190613,0.10705821216106415,-0.026308415457606316,-0.2514609396457672,0.27202290296554565,-0.13351190090179443,-0.22369295358657837,0.02349119260907173,0.31396225094795227,-0.015691392123699188,0.22528699040412903,-0.06939338892698288,-0.08504600822925568,0.16081932187080383,-0.16124559938907623,-0.08450357615947723,0.1442694216966629,0.23505157232284546,-0.03089349903166294,0.07006068527698517,0.03741326555609703,0.13767632842063904,-0.007382560055702925,-0.03575429692864418,0.13434471189975739,-0.15954451262950897,-0.1838369369506836,0.15126197040081024,-0.21083256602287292,-0.2659726142883301,-0.07703611999750137,0.15525124967098236,-0.19640031456947327,-0.2323792278766632,0.2516278326511383,-0.17185565829277039,-0.04286537691950798,-0.20452123880386353,-0.3668306767940521,0.25957435369491577,-0.2858557105064392,-0.2289164513349533,-0.016196241602301598,0.11989033967256546,-0.206110879778862,0.07609009742736816,-0.04439905658364296,0.2027377039194107,0.033262237906455994,-0.050290968269109726,-0.13979385793209076,-0.18656806647777557,-0.07535069435834885,-0.04279989376664162,0.2378794550895691,-0.08819323778152466,-0.11399178951978683,0.16111738979816437,0.23188187181949615,0.06697073578834534,0.1270376443862915,0.10653096437454224,-0.3351491689682007,0.18837006390094757,0.08120588958263397,-0.18819169700145721,0.010599489323794842,-0.15764367580413818,-0.0511510856449604,0.1766027957201004,0.09699123352766037,0.0791073888540268,-0.1447916328907013,-0.006329663097858429,0.07277791947126389,0.18804509937763214,0.11036713421344757,0.046589091420173645,-0.014763209968805313,0.09128312766551971,-0.021683065220713615,-0.17205223441123962,-0.018512943759560585,0.2517711818218231,-0.050035618245601654,-0.12607620656490326,-0.11831744760274887,0.25814464688301086,0.08587639033794403,0.08860845863819122,-0.2283928245306015,0.08047707378864288,-0.024796737357974052,0.05720502883195877,0.033028494566679,-0.13175739347934723,0.2653587758541107,-0.09687887132167816,0.07835620641708374,0.19080038368701935,-0.06009087339043617,0.05914034694433212,-0.05765218287706375,-0.05424008518457413,0.04012070223689079,-0.09965813905000687,0.013355935923755169,-0.037500590085983276,0.05700133740901947,0.2681051194667816,0.02314015105366707,-0.17326737940311432,0.006900102365761995,0.29510924220085144,0.0943603441119194,0.08742768317461014,-0.05530305206775665,0.24319496750831604,-0.01509284321218729,-0.06871204078197479,-0.2931833565235138,-0.30046817660331726,-0.1413014978170395,-0.01456463523209095,-0.08708850294351578,-0.018264206126332283,-0.03711102530360222,-0.17624431848526,-0.20924486219882965,-0.48099738359451294,0.0893741250038147,-0.18243026733398438,-0.15203745663166046,0.04706481844186783,0.031537823379039764,-0.2595762014389038,0.3108140230178833,0.20432959496974945,0.20874030888080597,-0.26128891110420227,0.01790815033018589,0.05954084172844887,-0.21059787273406982,-0.06204075366258621,-0.3393642008304596,0.13771751523017883,0.16341932117938995,0.07917379587888718,0.07797950506210327,-0.28340834379196167,-0.3712123930454254,0.1501673310995102,0.22736424207687378,0.21279072761535645,0.04989307373762131,0.0498773492872715,-0.06213489919900894,0.14426161348819733,0.09291587769985199,-0.0791238322854042,0.08736542612314224,-0.05023438110947609,-0.16863152384757996,-0.11466920375823975,0.01707259565591812,0.14574097096920013,-0.010704897344112396,0.15057869255542755,0.032424863427877426,-0.2402191311120987,0.16623015701770782,-0.10018561780452728,-0.06876452267169952,-0.21995094418525696,-0.10304169356822968,-0.3449574410915375,-0.10836165398359299,-0.03817182779312134,-0.10526425391435623,-0.1512419879436493,0.03700322285294533,0.22332721948623657,0.4023347496986389,0.33481186628341675,0.07076989859342575,0.07694961130619049,0.11404331028461456,0.05875306576490402,-0.052163396030664444,0.07495768368244171,0.05925653874874115,-0.11924860626459122,-0.05548516660928726,0.07836633920669556,-0.008227098733186722,0.07019145786762238,-0.2971024513244629,-0.3139152526855469,0.20588381588459015,-0.047431956976652145,-0.09617102891206741,0.0709831491112709,-0.10301578789949417,0.08758295327425003,0.04404924437403679,0.3655015528202057,0.07155155390501022,0.027951834723353386,-0.17095361649990082,0.12846094369888306,-0.21434837579727173,-0.03926236554980278,0.16936630010604858,-0.16813741624355316,0.012548415921628475,-0.07261408120393753,-0.009384636767208576,0.16955289244651794,-0.2740451693534851,-0.016285646706819534,0.0208602137863636,-0.05956031009554863,-0.19613602757453918,-0.09393873810768127,-0.14671623706817627,-0.2972296178340912,0.20848701894283295,-0.1535264402627945,0.016695450991392136,0.04088412970304489,-0.12841714918613434,0.22402967512607574,-0.20218047499656677,0.16385094821453094,0.07973244786262512,-0.08406761288642883,-0.0919574499130249,-0.07438008487224579,0.008816380985081196,-0.11012774705886841,-0.0433192253112793,-0.2600163221359253,0.08392506837844849,-0.11233700811862946,0.095351941883564,-0.21598701179027557,-0.05322282761335373,0.0566655732691288,0.26465949416160583,-0.13472838699817657,0.2094150334596634,-0.06456087529659271,-0.11666818708181381,-0.16748453676700592,-0.16581082344055176,0.06709592044353485,0.03159539774060249,-0.21071963012218475,-0.06829602271318436,-0.09532222896814346,0.01525133941322565,-0.06770296394824982,-0.059754855930805206,-0.0825079157948494,0.2502303123474121,0.16948264837265015,0.1545305848121643,0.32802072167396545,-0.0843445286154747,0.017466099932789803,0.08265240490436554,-0.25048646330833435,0.14279213547706604,0.15616677701473236,0.11954186111688614,-0.00572382053360343,0.02127676084637642,0.033954016864299774,-0.07031362503767014,-0.27007701992988586,-0.45937544107437134,0.2042749673128128,-0.24659956991672516,-0.1536971628665924,-0.08697202056646347,-0.03699370473623276,-0.16350439190864563,-0.11745060980319977,-0.06255689263343811,0.05023520439863205,0.32502105832099915,-0.06628337502479553,-0.08798634260892868,-0.18312811851501465,-0.30068889260292053,-0.11100272834300995,0.04571283608675003,-0.09132826328277588,0.03816702961921692,-0.0015029571950435638,0.003963884897530079,-0.1712445765733719,0.16271667182445526,0.16361607611179352,0.10912317782640457,-0.07893160730600357,-0.03944147005677223,0.22060944139957428,-0.13898979127407074,-0.21429656445980072,-0.3324604630470276,0.010894173756241798,-0.03815697878599167,0.18747137486934662,0.0989704355597496,-0.13883858919143677,-0.013864142820239067,0.17861072719097137,0.0646764263510704,-0.2513258755207062,0.1799105703830719,-0.003178791143000126,-0.026390554383397102,-0.18682225048542023,-0.4014483690261841,0.03131013363599777,-0.15611377358436584,-0.20370343327522278,-0.09195274114608765,0.11748538166284561,0.23885130882263184,0.2947051227092743,0.11201173812150955,-0.235687717795372,-0.13468961417675018,-0.07982023060321808,-0.017742644995450974,-0.1284424066543579,-0.11054348945617676,-0.02495819702744484,-0.1756274253129959,-0.1349814385175705,-0.06418927758932114,-0.054034020751714706,0.31336510181427,0.20142078399658203,-0.1003902330994606,0.08044266700744629,0.17656980454921722,-0.00990166887640953,-0.092491514980793,0.06921316683292389,-0.08073362708091736,-0.18150301277637482,0.4070119261741638,-0.048598889261484146,-0.02757590264081955,0.1537502557039261,-0.16085892915725708,-0.10744263976812363,-0.2718541920185089,-0.26709267497062683,0.2184663712978363,-0.21375995874404907,0.28318357467651367,0.3395117223262787,-0.018394675105810165,0.1888532191514969,0.06872666627168655,-0.037629466503858566,-0.009495891630649567,-0.13268861174583435,0.019117994233965874,-0.1470564305782318,0.04845881462097168,0.24801866710186005,0.2700843811035156,0.11604573577642441,0.2756851613521576,-0.051068734377622604,-0.032660577446222305,0.2653251886367798,-0.21685007214546204,0.07740498334169388,0.21433061361312866,0.014359455555677414,-0.10627260059118271,-0.07495654374361038,-0.11888160556554794,-0.039512891322374344,-0.15644292533397675,0.03660690411925316,-0.22977633774280548,0.12860259413719177,-0.11408689618110657,-0.018350474536418915,-0.0433560349047184,0.0969424769282341,0.12987296283245087,-0.09183963388204575,-0.07865425199270248,0.13770991563796997,0.20866332948207855,-0.020127426832914352,-0.2986413836479187,0.032094892114400864,-0.1416485458612442,-0.07276540249586105,0.030828053131699562,0.05028367415070534,-0.01890666037797928,0.021912239491939545,-0.2020106315612793,-0.09010669589042664,0.18056423962116241,-0.0849326029419899,0.11550260335206985,-0.07715547829866409,0.025809135288000107,-0.15030384063720703,0.13438504934310913,-0.040420837700366974,-0.006457220297306776,0.04408684000372887,0.05535973235964775,0.21909165382385254,-0.2024613916873932,0.024091005325317383,0.1460367888212204,0.15923522412776947,0.04635504260659218,0.013096875511109829,-0.09204737097024918,0.16959911584854126,-0.2925468981266022,-0.0844794362783432,0.06518369913101196,-0.01277010515332222,-0.2612408995628357,0.3128626346588135,0.13134686648845673,0.26249057054519653,0.014443137682974339,0.012753219343721867,-0.2856793701648712,0.24523790180683136,-0.011199350468814373,0.1131216436624527,-0.1293627917766571,0.3705229163169861,0.18357759714126587,0.007256441283971071,0.026906803250312805,-0.1502676159143448,-0.258255273103714,0.06350089609622955,0.13425160944461823,-0.09396989643573761,-0.17338034510612488,0.19160057604312897,0.023575391620397568,0.0072123329155147076,-0.2026912271976471,0.17301452159881592,-0.06083446741104126,0.06489741057157516,-0.027284780517220497,0.09994920343160629,-0.21455661952495575,0.13812768459320068,0.07529240846633911,-0.08398468047380447,-0.0496625155210495,-0.1429957151412964,0.055111389607191086,0.07024578750133514,-0.006367174908518791,-0.09643733501434326,-0.07304641604423523,0.1874152570962906,0.03462185710668564,0.0730796679854393,0.13275542855262756,-0.02280915342271328,-0.048321645706892014,-0.09927212446928024,0.0683579221367836,-0.180110365152359,0.0396902821958065,0.018498685210943222,-0.06533751636743546,0.19232794642448425,-0.0471474826335907,0.07575130462646484,-0.027692411094903946,0.026058170944452286,-0.012610292993485928,-0.026401415467262268,0.12818948924541473,0.02694224938750267,-0.023614386096596718,-0.16348138451576233,-0.2133566439151764,-0.010269185528159142,-0.029086217284202576,0.11152783781290054,0.13616551458835602,-0.03306927531957626,0.06933710724115372,0.20811665058135986,0.048096757382154465,0.07929515093564987,-0.13040810823440552,-0.08830519765615463,0.019704604521393776,-0.0590442530810833,-0.1339993178844452,-0.12018559873104095,-0.013408537022769451,-0.17207878828048706,-0.07380601763725281,-0.08962562680244446,0.07956279069185257,0.05628309026360512,0.12996211647987366,0.3009171485900879,0.0011784440139308572,0.03643142059445381,0.06509266793727875,-0.11822042614221573,-0.1659630686044693,-0.10614217072725296,0.013204219751060009,0.11616477370262146,-0.05712936818599701,0.07341285049915314,0.052282918244600296,0.001557283685542643,0.001513639115728438,-0.05260368809103966,-0.2394455075263977,-0.05526375398039818,-0.21770325303077698,-0.04858444258570671,-0.3303306996822357,0.2225269079208374,-0.015298177488148212,-0.08740922808647156,0.050958890467882156,0.040569815784692764,0.1418701857328415,-0.08141002804040909,0.04461004585027695,0.11165137588977814,0.3463270366191864,0.12561699748039246,-0.23691999912261963,0.005188196897506714,-0.08553186058998108,0.05013898387551308,-0.01824921742081642,-0.03360786661505699,-0.36509668827056885,-0.053484369069337845,0.10461681336164474,-0.02318260446190834,-0.023705841973423958,0.10491656512022018,-0.05790358781814575,0.038414884358644485,-0.13048076629638672,0.062237270176410675,-0.08944721519947052,0.0008872078615240753,-0.24847614765167236,-0.02626754902303219,-0.04246728867292404,0.052344102412462234,0.0022030065301805735,-0.27908003330230713,0.11242804676294327]}}}\' -vv')

